{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: Your name*\n",
      "label: Your email*\n",
      "label: Your number*\n",
      "label: City of the Pizza Hut*\n",
      "label: Select Store*\n",
      "label: Order Type*\n",
      "label: Dine-in\n",
      "label: Take Away\n",
      "label: Delivery\n",
      "label: Order Method*\n",
      "label: At the Store\n",
      "label: Pizza Hut (APP or Web)\n",
      "label: Aggregator (Swiggy/Zomato)\n",
      "label: Time Of Visit*\n",
      "label: Date of Visit*\n",
      "label: Your Order Id\n",
      "label: Appreciation\n",
      "label: Complaint\n",
      "label: Your Feedback*\n",
      "input_placeholder: Enter Name\n",
      "input_name: Name\n",
      "input_placeholder: Enter email address\n",
      "input_name: Email Address\n",
      "input_placeholder: Enter phone number\n",
      "input_name: Mobile Number\n",
      "input_name: Order Type\n",
      "input_name: Order Type\n",
      "input_name: Order Type\n",
      "input_name: Order Method\n",
      "input_name: Order Method\n",
      "input_name: Order Method\n",
      "input_placeholder: Select Date\n",
      "input_name: Date Of Visit\n",
      "input_placeholder: Order ID\n",
      "input_name: Order Id\n",
      "input_name: Staff Recognition\n",
      "input_name: Staff Recognition\n",
      "input_name: Submitbtn\n",
      "select: City Id\n",
      "select: Store Id\n",
      "select: Time Of Visit\n",
      "select: Complaint Category\n",
      "textarea_placeholder: Enter your feedback\n",
      "textarea_name: Your Feedback\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_form_fields(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    form_fields = []\n",
    "\n",
    "    # Function to clean text\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\s*(\\*\\[ Required \\]\\*|is Required).*$', '', text)\n",
    "        text = re.sub(r'--\\s*.*\\s*--', '', text).strip()\n",
    "        return text\n",
    "\n",
    "    # Extract from labels\n",
    "    for label in soup.find_all('label'):\n",
    "        text = clean_text(label.get_text())\n",
    "        if text and not text.startswith('*'):\n",
    "            form_fields.append(('label', text))\n",
    "\n",
    "    # Extract from inputs\n",
    "    for input_tag in soup.find_all('input'):\n",
    "        placeholder = input_tag.get('placeholder')\n",
    "        if placeholder:\n",
    "            placeholder = clean_text(placeholder)\n",
    "            if placeholder and not placeholder.startswith('*'):\n",
    "                form_fields.append(('input_placeholder', placeholder))\n",
    "        \n",
    "        name = input_tag.get('name')\n",
    "        if name:\n",
    "            name = clean_text(name.replace('_', ' ').title())\n",
    "            if name and not name.startswith('*'):\n",
    "                form_fields.append(('input_name', name))\n",
    "\n",
    "    # Extract from selects\n",
    "    for select in soup.find_all('select'):\n",
    "        name = select.get('name')\n",
    "        if name:\n",
    "            name = clean_text(name.replace('_', ' ').title())\n",
    "            if name and not name.startswith('*'):\n",
    "                form_fields.append(('select', name))\n",
    "\n",
    "    # Extract from textareas\n",
    "    for textarea in soup.find_all('textarea'):\n",
    "        placeholder = textarea.get('placeholder')\n",
    "        if placeholder:\n",
    "            placeholder = clean_text(placeholder)\n",
    "            if placeholder and not placeholder.startswith('*'):\n",
    "                form_fields.append(('textarea_placeholder', placeholder))\n",
    "        \n",
    "        name = textarea.get('name')\n",
    "        if name:\n",
    "            name = clean_text(name.replace('_', ' ').title())\n",
    "            if name and not name.startswith('*'):\n",
    "                form_fields.append(('textarea_name', name))\n",
    "\n",
    "    return form_fields\n",
    "\n",
    "# Example usage\n",
    "url='https://feedback.pizzahut.co.in/'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "html_content = requests.get(url,headers=headers).text\n",
    "result = extract_form_fields(html_content)\n",
    "for field_type, field_name in result:\n",
    "    print(f\"{field_type}: {field_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "understanding regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "import requests\n",
    "url='https://feedback.pizzahut.co.in/'\n",
    "url2='https://usa-web.scansoftware.com/cafeweb/tapestry?page=Application'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "html_content = requests.get(url2,headers=headers).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First NameRequired\n",
      "Middle NameRequired\n",
      "Last NameRequired\n",
      "Email AddressRequired\n",
      "Social Security NumberRequired\n",
      "Date of BirthRequired\n",
      "GenderRequired\n",
      "Are you Hispanic or Latino?Required\n",
      "Racial BackgroundRequired\n",
      "White\n",
      "Black or African American\n",
      "Asian\n",
      "American Indian or Alaska Native\n",
      "Native Hawaiian or Other Pacific Islander\n",
      "Citizenship StatusRequired\n",
      "I do not wish to receive text messages.Required\n",
      "CountryRequired\n",
      "Street 1Required\n",
      "Street 2Required\n",
      "CityRequired\n",
      "StateRequired\n",
      "Zip CodeRequired\n",
      "Zip Code\tExtension\n",
      "ProvinceRequired\n",
      "Postal CodeRequired\n",
      "Alternate PhoneRequired\n",
      "Alternate Phone\tArea Code\n",
      "Alternate Phone\tExchange\n",
      "Alternate Phone\tNumber\n",
      "Alternate Phone\tExtension\n",
      "Mobile PhoneRequired\n",
      "Mobile Phone\tArea Code\n",
      "Mobile Phone\tExchange\n",
      "Mobile Phone\tNumber\n",
      "Mobile Phone\tExtension\n",
      "International Mobile PhoneRequired\n",
      "International Mobile Phone Extension\n",
      "Highest Level of EducationRequired\n",
      "HS/GED Graduation YearRequired\n",
      " High School Name / GED InstituteRequired\n",
      "Post-Secondary School attended, if applicable.Required\n",
      "Post-Secondary School attended, if applicable.Required\n",
      "Program of StudyRequired\n",
      "Anticipated Starting SemesterRequired\n",
      "Are you a U.S. Veteran?Required\n",
      "Payment MethodRequired\n",
      "How did you hear about United States Sports Academy?Required\n",
      "Years AttendedRequired\n",
      "Signature (Please type your full name)Required\n",
      "Today's Date (mm/dd/yyyy)Required\n",
      "Prior Degree, if applicableRequired\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_form_fields(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    form_fields = []\n",
    "\n",
    "    # Function to clean text\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\s*(\\*\\[ Required \\]\\*|is Required).*$', '', text)\n",
    "        text = re.sub(r'--\\s*.*\\s*--', '', text).strip()\n",
    "        return text\n",
    "\n",
    "    # Extract from labels\n",
    "    for label in soup.find_all('label'):\n",
    "        print(label.get_text())\n",
    "        # text = clean_text(label.get_text())\n",
    "\n",
    "        # if text and not text.startswith('*'):\n",
    "        #     form_fields.append(('label', text))\n",
    "\n",
    "    # Extract from inputs\n",
    "    # for input_tag in soup.find_all('input'):\n",
    "    #     placeholder = input_tag.get('placeholder')\n",
    "    #     if placeholder:\n",
    "    #         placeholder = clean_text(placeholder)\n",
    "    #         if placeholder and not placeholder.startswith('*'):\n",
    "    #             form_fields.append(('input_placeholder', placeholder))\n",
    "        \n",
    "    #     name = input_tag.get('name')\n",
    "    #     if name:\n",
    "    #         name = clean_text(name.replace('_', ' ').title())\n",
    "    #         if name and not name.startswith('*'):\n",
    "    #             form_fields.append(('input_name', name))\n",
    "\n",
    "    # # Extract from selects\n",
    "    # for select in soup.find_all('select'):\n",
    "    #     name = select.get('name')\n",
    "    #     if name:\n",
    "    #         name = clean_text(name.replace('_', ' ').title())\n",
    "    #         if name and not name.startswith('*'):\n",
    "    #             form_fields.append(('select', name))\n",
    "\n",
    "    # Extract from textareas\n",
    "    # for textarea in soup.find_all('textarea'):\n",
    "    #     placeholder = textarea.get('placeholder')\n",
    "    #     if placeholder:\n",
    "    #         placeholder = clean_text(placeholder)\n",
    "    #         if placeholder and not placeholder.startswith('*'):\n",
    "    #             form_fields.append(('textarea_placeholder', placeholder))\n",
    "        \n",
    "    #     name = textarea.get('name')\n",
    "    #     if name:\n",
    "    #         name = clean_text(name.replace('_', ' ').title())\n",
    "    #         if name and not name.startswith('*'):\n",
    "    #             form_fields.append(('textarea_name', name))\n",
    "\n",
    "    return form_fields\n",
    "\n",
    "res=extract_form_fields(html_content)\n",
    "for ele in res:print(ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "improved version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from thefuzz import fuzz\n",
    "\n",
    "def load_nlp_model():\n",
    "    try:\n",
    "        return spacy.load(\"en_core_web_sm\")\n",
    "    except:\n",
    "        print(\"Downloading spaCy model...\")\n",
    "        spacy.cli.download(\"en_core_web_sm\")\n",
    "        return spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp = load_nlp_model()\n",
    "\n",
    "def is_similar_to_required(word, threshold=80):\n",
    "    return fuzz.ratio(word.lower(), \"required\") > threshold\n",
    "\n",
    "def clean_text_and_check_required(text):\n",
    "    original_text = text\n",
    "    \n",
    "    # Check for asterisk anywhere in the text\n",
    "    has_asterisk = '*' in text\n",
    "    text = text.replace('*', '').strip()\n",
    "    \n",
    "    # Use NLP to check for words similar to \"required\"\n",
    "    doc = nlp(text)\n",
    "    required_words = []\n",
    "    for token in doc:\n",
    "        if is_similar_to_required(token.text) or token.text.lower() in [\"mandatory\", \"compulsory\"]:\n",
    "            required_words.append(token.text)\n",
    "    \n",
    "    # Remove identified required words\n",
    "    for word in required_words:\n",
    "        text = text.replace(word, '').strip()\n",
    "    \n",
    "    # Final cleaning\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'--\\s*.*\\s*--', '', text).strip()\n",
    "    text = re.sub(r'[\\(\\[\\{].*?[\\)\\]\\}]', '', text).strip()  # Remove content in brackets\n",
    "    \n",
    "    is_required = has_asterisk or len(required_words) > 0\n",
    "    \n",
    "    return text, is_required, original_text\n",
    "\n",
    "# Example usage\n",
    "example_texts = [\n",
    "    \"First Name *\",\n",
    "    \"* Last Name\",\n",
    "    \"Email Address (Required)\",\n",
    "    \"Required: Phone Number\",\n",
    "    \"Address -- This field is compulsory --\",\n",
    "    \"Comments (Optional)\",\n",
    "    \"Mandatory: Date of Birth\",\n",
    "    \"{Required} Social Security Number\",\n",
    "    \"First Name*\",\"*First Name\",\"[Required]First Name\"\n",
    "]\n",
    "# Less chances it will be able to handle  \n",
    "for example in example_texts:\n",
    "    cleaned_text, required, original = clean_text_and_check_required(example)\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Cleaned: {cleaned_text}\")\n",
    "    print(f\"Required: {required}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First\n",
      "Name\n",
      "*\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "def load_nlp_model():\n",
    "    try:\n",
    "        return spacy.load(\"en_core_web_sm\")\n",
    "    except:\n",
    "        print(\"Downloading spaCy model...\")\n",
    "        spacy.cli.download(\"en_core_web_sm\")\n",
    "        return spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp = load_nlp_model()\n",
    "doc=nlp(\"First Name *\")\n",
    "for ele in doc:print(ele)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
